{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a00f0-94f9-4027-9787-2bf63eec971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "# pip install nltk\n",
    "import nltk \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2324b4b-961f-40dd-a6e1-ffe84e29ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you curious about tokenizer? let's see how it works! we need to analize a couple\n",
      "['Are you curious about tokenizer?', \"let's see how it works!\", 'we need to analize a couple']\n"
     ]
    }
   ],
   "source": [
    "# Sent tokenizer\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "text = \"Are you curious about tokenizer? let's see how it works! we need to analize a couple\"\n",
    "print(text)\n",
    "output = sent_tokenize(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da26a470-3c52-4c79-92de-7ae8462b5bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you curious about tokenizer? let's see how it works! we need to analize a couple\n",
      "['Are you curious about tokenizer?', \"let's see how it works!\", 'we need to analize a couple']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenizer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "text = \"Are you curious about tokenizer? let's see how it works! we need to analize a couple\"\n",
    "print(text)\n",
    "output = sent_tokenize(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e01b500a-b66e-41e7-a9ea-fe379980cadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'curious', 'about', 'tokenizer', '?', 'let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'we', 'need', 'to', 'analize', 'a', 'couple']\n"
     ]
    }
   ],
   "source": [
    "# WordPunct tokenizer\n",
    "\n",
    "from nltk import wordpunct_tokenize\n",
    "text = \"Are you curious about tokenizer? let's see how it works! we need to analize a couple\"\n",
    "output = wordpunct_tokenize(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4dc0d44-3880-4a3c-b86d-fe095f0bf481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table ['tabl'] ['tabl'] ['tabl']\n",
      "probably ['probabl'] ['prob'] ['probabl']\n",
      "wolves ['wolv'] ['wolv'] ['wolv']\n",
      "playing ['play'] ['play'] ['play']\n",
      "is ['is'] ['is'] ['is']\n",
      "cats ['cat'] ['cat'] ['cat']\n",
      "the ['the'] ['the'] ['the']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "words = ['table','probably','wolves','playing','is','cats','the']\n",
    "\n",
    "# easy to use\n",
    "stemmer_porter = PorterStemmer()\n",
    "\n",
    "# complex to small words\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "# multilangual\n",
    "snowball_stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "for word in words :\n",
    "    stemmed_words = [stemmer_porter.stem(word)]\n",
    "    lancaster_porter = [lancaster_stemmer.stem(word)]\n",
    "    snowball_porter = [snowball_stemmer.stem(word)]\n",
    "    print(word,stemmed_words,lancaster_porter,snowball_porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2e06b6-1d10-4753-b5ee-f6914cf34ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table --> table table\n",
      "probably --> probably probably\n",
      "wolves --> wolves wolf\n",
      "playing --> play playing\n",
      "is --> be is\n",
      "cats --> cat cat\n",
      "the --> the the\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words = ['table','probably','wolves','playing','is','cats','the']\n",
    "lwordnet = WordNetLemmatizer()\n",
    "for word in words :\n",
    "    lemmatized_words = [lwordnet.lemmatize(word,pos = \"v\"),lwordnet.lemmatize(word,pos = \"n\")]\n",
    "    print(word,\"-->\",*lemmatized_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
